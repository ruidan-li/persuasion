{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-F-Tn-UZ-ruE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-F-Tn-UZ-ruE",
    "outputId": "7709b1e5-123f-46bf-e4fd-4a479cb22469"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# DATA_DIR = \"./drive/MyDrive/NLP/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "CRiDGzF8-3xi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRiDGzF8-3xi",
    "outputId": "87d0076b-7026-49fc-b288-c061e64eb905"
   },
   "outputs": [],
   "source": [
    "# !pip install tqdm boto3 requests regex sentencepiece sacremoses\n",
    "# !pip install transformers\n",
    "# !pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4096c0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation-models-pytorch\n",
      "  Using cached segmentation_models_pytorch-0.2.1-py3-none-any.whl (88 kB)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages (from segmentation-models-pytorch) (0.11.3+cu102)\n",
      "Collecting pretrainedmodels==0.7.4\n",
      "  Using cached pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
      "Collecting efficientnet-pytorch==0.6.3\n",
      "  Using cached efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n",
      "Collecting timm==0.4.12\n",
      "  Using cached timm-0.4.12-py3-none-any.whl (376 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (1.10.2+cu102)\n",
      "Collecting munch\n",
      "  Using cached munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (4.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.22.1)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (9.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm->pretrainedmodels==0.7.4->segmentation-models-pytorch) (0.4.4)\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
      "  Building wheel for efficientnet-pytorch (setup.py): started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\larry\\anaconda3\\envs\\nlp\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for efficientnet-pytorch (setup.py): finished with status 'done'\n",
      "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12421 sha256=97b9529212c67261699ece3cca5caa7476bbc86b3d712ec041defd8cdb837306\n",
      "  Stored in directory: c:\\users\\larry\\appdata\\local\\pip\\cache\\wheels\\70\\f8\\49\\20f330df3f946fed839df657dd2156c929d6d7b5f774d9650e\n",
      "  Building wheel for pretrainedmodels (setup.py): started\n",
      "  Building wheel for pretrainedmodels (setup.py): finished with status 'done'\n",
      "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=390b1450f7b6ac22d23a6695f977f668534252aac3339f85495eba71a24415a9\n",
      "  Stored in directory: c:\\users\\larry\\appdata\\local\\pip\\cache\\wheels\\d1\\3b\\4e\\2f3015f1ab76f34be28e04c4bcee27e8cabfa70d2eadf8bc3b\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\n",
      "Installing collected packages: munch, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.2.1 timm-0.4.12\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc1c45a2",
   "metadata": {
    "id": "fc1c45a2"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "import tarfile\n",
    "import os.path\n",
    "import json\n",
    "import re\n",
    "from bz2 import BZ2File\n",
    "from urllib import request\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = \"\"\n",
    "fname = \"cmv.tar.bz2\"\n",
    "# url = \"https://chenhaot.com/data/cmv/\" + fname\n",
    "\n",
    "# download if not exists\n",
    "# if not os.path.isfile(fname):\n",
    "#     f = BytesIO()\n",
    "#     with request.urlopen(url) as resp, open(fname, 'wb') as f_disk:\n",
    "#         data = resp.read()\n",
    "#         f_disk.write(data)  # save to disk too\n",
    "#         f.write(data)\n",
    "#         f.seek(0)\n",
    "# else:\n",
    "f = open(DATA_DIR+fname, 'rb')\n",
    "\n",
    "\n",
    "tar = tarfile.open(fileobj=f, mode=\"r\")\n",
    "\n",
    "# Extract the file we are interested in\n",
    "\n",
    "train_fname = \"op_task/train_op_data.jsonlist.bz2\"\n",
    "test_fname = \"op_task/heldout_op_data.jsonlist.bz2\"\n",
    "\n",
    "all_train_fname = \"all/train_period_data.jsonlist.bz2\"\n",
    "all_test_fname = \"all/heldout_period_data.jsonlist.bz2\"\n",
    "\n",
    "pair_train_fname = \"pair_task/train_pair_data.jsonlist.bz2\"\n",
    "pair_test_fname = \"pair_task/train_pair_data.jsonlist.bz2\"\n",
    "\n",
    "train_bzlist = tar.extractfile(train_fname)\n",
    "# all_train_bzlist = tar.extractfile(all_train_fname)\n",
    "# pair_train_bzlist = tar.extractfile(pair_train_fname)\n",
    "\n",
    "# Deserialize the JSON list\n",
    "original_posts_train = [\n",
    "    json.loads(line.decode('utf-8'))\n",
    "    for line in BZ2File(train_bzlist)\n",
    "]\n",
    "# all_train = [\n",
    "#     json.loads(line.decode('utf-8'))\n",
    "#     for line in BZ2File(all_train_bzlist)\n",
    "# ]\n",
    "# pair_train = [\n",
    "#     json.loads(line.decode('utf-8'))\n",
    "#     for line in BZ2File(pair_train_bzlist)\n",
    "# ]\n",
    "test_bzlist = tar.extractfile(test_fname)\n",
    "# all_test_bzlist = tar.extractfile(all_test_fname)\n",
    "# pair_test_bzlist = tar.extractfile(pair_test_fname)\n",
    "\n",
    "original_posts_test = [\n",
    "    json.loads(line.decode('utf-8'))\n",
    "    for line in BZ2File(test_bzlist)\n",
    "]\n",
    "# all_test = [\n",
    "#     json.loads(line.decode('utf-8'))\n",
    "#     for line in BZ2File(all_test_bzlist)\n",
    "# ]\n",
    "# pair_test = [\n",
    "#     json.loads(line.decode('utf-8'))\n",
    "#     for line in BZ2File(pair_test_bzlist)\n",
    "# ]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd8f4a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dd8f4a9",
    "outputId": "15ed0b2e-1f89-40cc-c004-89515f97486d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10743, 3191\n"
     ]
    }
   ],
   "source": [
    "malleable_op = [post[\"name\"] for post in original_posts_train if post[\"delta_label\"]]\n",
    "print(f'{len(original_posts_train)}, {len(malleable_op)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c97f43",
   "metadata": {
    "id": "e7c97f43"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "G3EXsUgSBcQe",
   "metadata": {
    "id": "G3EXsUgSBcQe"
   },
   "outputs": [],
   "source": [
    "def cleanup(cmv_post):\n",
    "    lines = [line for line in cmv_post.splitlines()\n",
    "             if not line.lstrip().startswith(\"&gt;\")\n",
    "             and not line.lstrip().startswith(\"____\")\n",
    "             and \"edit\" not in \" \".join(line.lower().split()[:2])\n",
    "            ]\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "721e4fdc",
   "metadata": {
    "id": "721e4fdc"
   },
   "outputs": [],
   "source": [
    "op_text_train = [\n",
    "    cleanup(original_post['selftext'])\n",
    "    # original_post['selftext']  # try this at home\n",
    "    for original_post\n",
    "    in original_posts_train\n",
    "]\n",
    "op_text_test = [\n",
    "    cleanup(original_post['selftext'])\n",
    "    # original_post['selftext']  # try this at home\n",
    "    for original_post\n",
    "    in original_posts_test\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1179bede",
   "metadata": {
    "id": "1179bede"
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(use_idf=False, norm='l1')\n",
    "X_train = vect.fit_transform(op_text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aede9fd3",
   "metadata": {
    "id": "aede9fd3"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(Cs=10, class_weight='balanced',\n",
    "                          scoring='roc_auc', solver='sag',\n",
    "                          tol=0.001, max_iter=500,\n",
    "                          random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "877a4ef5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "877a4ef5",
    "outputId": "5dffa386-3991-4c9a-dc22-fbc1d5d44a56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10743, 48296)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78539c4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78539c4b",
    "outputId": "51339d7e-eb89-4b8b-d2a7-a183a1ebd465"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(class_weight='balanced', max_iter=500, random_state=0,\n",
       "                     scoring='roc_auc', solver='sag', tol=0.001)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_malleable_train = np.array([post[\"delta_label\"] for post in original_posts_train])\n",
    "\n",
    "lr.fit(X_train, is_malleable_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75163de3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75163de3",
    "outputId": "b8e5cdb8-e5aa-4ad8-b068-291bfeb33643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC score: 0.531\n"
     ]
    }
   ],
   "source": [
    "X_test = vect.transform(op_text_test)\n",
    "is_malleable_test = np.array([post[\"delta_label\"] for post in original_posts_test])\n",
    "test_roc = roc_auc_score(is_malleable_test,lr.predict_proba(X_test)[:,1])\n",
    "print(\"Test ROC AUC score: {:.3f}\".format(test_roc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c350ada",
   "metadata": {
    "id": "9c350ada"
   },
   "source": [
    "## Longformer Tokenization + UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "041ab25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments, LongformerConfig\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15dd9346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"attention_window\": 512,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"longformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"sep_token_id\": 2,\n",
       "  \"transformers_version\": \"4.17.0.dev0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = LongformerConfig()\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afa1658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and tokenizer and define length of the text sequence\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba880997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0, 100, 206,  ...,   1,   1,   1]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "example_text = op_text_train[0]\n",
    "long_input = tokenizer(example_text,padding='max_length', max_length = 1024, \n",
    "                       truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(long_input['input_ids'])\n",
    "print(long_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "defcd299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = [int(label) for label in df['label']]\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 1024, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "480c53c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "is_malleable_train = np.array([post[\"delta_label\"] for post in original_posts_train])\n",
    "is_malleable_test = np.array([post[\"delta_label\"] for post in original_posts_test])\n",
    "df_train_total = pd.DataFrame(list(zip(op_text_train,is_malleable_train)),columns = ['text','label'])\n",
    "df_train, df_val = np.split(df_train_total.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df_train_total))])\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = pd.DataFrame(list(zip(op_text_test,is_malleable_test)),columns = ['text','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "470788d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing data\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizing data\")\n",
    "train = Dataset(df_train)\n",
    "val = Dataset(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "90553a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=False)\n",
    "val_dataloader = torch.utils.data.DataLoader(val, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6f18a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "class UNET_Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(UNET_Classifier, self).__init__()\n",
    "\n",
    "        self.unet = smp.Unet(\n",
    "            encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "            encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "            in_channels=2,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "            classes=1,                      # model output channels (number of classes in your dataset)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.linear = nn.Linear(256,1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "            \n",
    "        input_2d = input_id.reshape((-1,1,32,32)).float()\n",
    "        mask_2d = mask.reshape((-1,1,32,32)).float()\n",
    "        final_input = torch.cat((input_2d,mask_2d),dim=1)\n",
    "        out = self.unet(final_input)\n",
    "        out = self.pool(out)\n",
    "        out = self.relu(out)\n",
    "        out= out.reshape((-1,256))\n",
    "        out = self.linear(out)\n",
    "        out = self.sigmoid(out).squeeze()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "47e08d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(train, val, lr=0.001, BATCH_SIZE=16, num_epochs = 50):\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model = UNET_Classifier()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = Adam(model.parameters(), lr= lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "#     print(\"start training\")\n",
    "\n",
    "    best_val_loss = 100\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    for epoch_num in range(num_epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in train_dataloader:\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                batch_loss = criterion(output.squeeze(), train_label.float())\n",
    "                total_loss_train += batch_loss.item()\n",
    "\n",
    "                acc = (torch.round(output) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for val_input, val_label in val_dataloader:\n",
    "\n",
    "            val_label = val_label.to(device)\n",
    "            mask = val_input['attention_mask'].to(device)\n",
    "            input_id = val_input['input_ids'].to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            batch_loss = criterion(output.squeeze(), val_label.float())\n",
    "            total_loss_val += batch_loss.item()\n",
    "\n",
    "            acc = (torch.round(output) == val_label).sum().item()\n",
    "            total_acc_val += acc\n",
    "\n",
    "#             print(\n",
    "#                 f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train): .3f} \\\n",
    "#                 | Train Accuracy: {total_acc_train / len(train): .3f} \\\n",
    "#                 | Val Loss: {total_loss_val / len(val): .3f} \\\n",
    "#                 | Val Accuracy: {total_acc_val / len(val): .3f}')\n",
    "#             if (total_loss_val/len(val))<best_val_loss:\n",
    "#                 best_val_loss = total_loss_val/len(val)\n",
    "#                 print(\"New best val loss, model saved!\")\n",
    "#                 torch.save(model,'best_model_unet')\n",
    "    return total_acc_val / len(val), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "277513dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I think the world is automating fast enough th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>First off, I do not believe that Iran *should*...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please leave the footnote below the following ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It seems to me that it is entirely justified t...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The, \"Under God\" line is actually a relatively...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10738</th>\n",
       "      <td>Muhammad did not prohibit paintings of him bec...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10739</th>\n",
       "      <td>It seems logical to me that school hours shoul...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10740</th>\n",
       "      <td>I think that dynastic wealth i.e. passing down...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10741</th>\n",
       "      <td>cars are a bad idea. just like oil is a bad id...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10742</th>\n",
       "      <td>My argument assumes the Christian theology tau...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10743 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I think the world is automating fast enough th...  False\n",
       "1      First off, I do not believe that Iran *should*...  False\n",
       "2      Please leave the footnote below the following ...  False\n",
       "3      It seems to me that it is entirely justified t...  False\n",
       "4      The, \"Under God\" line is actually a relatively...  False\n",
       "...                                                  ...    ...\n",
       "10738  Muhammad did not prohibit paintings of him bec...  False\n",
       "10739  It seems logical to me that school hours shoul...   True\n",
       "10740  I think that dynastic wealth i.e. passing down...  False\n",
       "10741  cars are a bad idea. just like oil is a bad id...  False\n",
       "10742  My argument assumes the Christian theology tau...   True\n",
       "\n",
       "[10743 rows x 2 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "dd5ee1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I think the world is automating fast enough th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>First off, I do not believe that Iran *should*...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please leave the footnote below the following ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It seems to me that it is entirely justified t...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The, \"Under God\" line is actually a relatively...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10735</th>\n",
       "      <td>The rate at which income is taxed (at least in...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10736</th>\n",
       "      <td>Charlie Hebdo is staffed exclusively by white,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10737</th>\n",
       "      <td>Certain limitations of freedom of speech are c...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10738</th>\n",
       "      <td>Muhammad did not prohibit paintings of him bec...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10739</th>\n",
       "      <td>It seems logical to me that school hours shoul...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10740 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I think the world is automating fast enough th...  False\n",
       "1      First off, I do not believe that Iran *should*...  False\n",
       "2      Please leave the footnote below the following ...  False\n",
       "3      It seems to me that it is entirely justified t...  False\n",
       "4      The, \"Under God\" line is actually a relatively...  False\n",
       "...                                                  ...    ...\n",
       "10735  The rate at which income is taxed (at least in...   True\n",
       "10736  Charlie Hebdo is staffed exclusively by white,...  False\n",
       "10737  Certain limitations of freedom of speech are c...  False\n",
       "10738  Muhammad did not prohibit paintings of him bec...  False\n",
       "10739  It seems logical to me that school hours shoul...   True\n",
       "\n",
       "[10740 rows x 2 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv = df_train_total[:10740]\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "70156835",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int(len(df_cv)/5)\n",
    "max_val_acc = 0\n",
    "file_name = 'unet_results.txt'\n",
    "\n",
    "def run_CV(df_train, lr, BATCH_SIZE, NUM_EPOCHS, max_val_acc):   \n",
    "    final_acc_val = np.zeros(5)\n",
    "    print(\"lr =\", lr, \", batch size =\", BATCH_SIZE,\", num_epochs =\", NUM_EPOCHS)\n",
    "    print(\"lr =\", lr, \", batch size =\", BATCH_SIZE, \", num_epochs =\", NUM_EPOCHS, file=open(file_name, \"a\"))\n",
    "    for i in range(5):\n",
    "        \n",
    "        train_temp = pd.concat([df_train[0:i*val_size],df_train[(i+1)*val_size:]]).reset_index(drop=True)\n",
    "        val_temp = df_train[i*val_size:(i+1)*val_size].reset_index(drop=True)\n",
    "        train = Dataset(train_temp)\n",
    "        val = Dataset(val_temp)\n",
    "        val_acc, model = run_test(train, val, lr=lr, BATCH_SIZE=BATCH_SIZE, num_epochs = NUM_EPOCHS)\n",
    "        final_acc_val[i] = val_acc\n",
    "        \n",
    "#         losses, losses_val, model  = run_test(X_train_temp,y_train_temp,X_val_temp,y_val_temp,TASK=TASK, lr=lr, \n",
    "#                                       BATCH_SIZE=BATCH_SIZE, num_epochs=NUM_EPOCHS)\n",
    "#         final_loss_train[i] = losses[-1]\n",
    "#         final_loss_val[i] = losses_val[-1]\n",
    "        \n",
    "    \n",
    "    print(\"average validation acc:\", final_acc_val.mean())\n",
    "    print(\"average validation acc:\", final_acc_val.mean(), file=open(file_name, \"a\"))\n",
    "    \n",
    "    if final_acc_val.mean() > max_val_acc:\n",
    "        max_val_acc = final_acc_val.mean()\n",
    "        print('best params:',  \"lr =\", lr, \", batch size =\", BATCH_SIZE, \", num_epochs =\", NUM_EPOCHS)\n",
    "        print('best params:',  \"lr =\", lr, \", batch size =\", BATCH_SIZE, \", num_epochs =\", NUM_EPOCHS, file=open(file_name, \"a\"))\n",
    "        torch.save(model,'best_model_unet')\n",
    "        print('new best model saved!')\n",
    "    return max_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "92d4990d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.0001 , batch size = 8 , num_epochs = 5\n",
      "average validation acc: 0.7027001862197393\n",
      "best params: lr = 0.0001 , batch size = 8 , num_epochs = 5\n",
      "new best model saved!\n",
      "lr = 0.0001 , batch size = 8 , num_epochs = 10\n",
      "average validation acc: 0.6965549348230912\n",
      "lr = 0.0001 , batch size = 8 , num_epochs = 20\n",
      "average validation acc: 0.6182495344506518\n",
      "lr = 0.0001 , batch size = 8 , num_epochs = 50\n",
      "average validation acc: 0.6116387337057729\n",
      "lr = 0.0001 , batch size = 16 , num_epochs = 5\n",
      "average validation acc: 0.6971135940409683\n",
      "lr = 0.0001 , batch size = 16 , num_epochs = 10\n",
      "average validation acc: 0.639292364990689\n",
      "lr = 0.0001 , batch size = 16 , num_epochs = 20\n",
      "average validation acc: 0.5952513966480447\n",
      "lr = 0.0001 , batch size = 16 , num_epochs = 50\n",
      "average validation acc: 0.5844506517690875\n",
      "lr = 0.0001 , batch size = 32 , num_epochs = 5\n",
      "average validation acc: 0.6818435754189944\n",
      "lr = 0.0001 , batch size = 32 , num_epochs = 10\n",
      "average validation acc: 0.5938547486033519\n",
      "lr = 0.0001 , batch size = 32 , num_epochs = 20\n",
      "average validation acc: 0.5616387337057728\n",
      "lr = 0.0001 , batch size = 32 , num_epochs = 50\n",
      "average validation acc: 0.5533519553072626\n",
      "lr = 0.0001 , batch size = 64 , num_epochs = 5\n",
      "average validation acc: 0.6534450651769087\n",
      "lr = 0.0001 , batch size = 64 , num_epochs = 10\n",
      "average validation acc: 0.5987895716945996\n",
      "lr = 0.0001 , batch size = 64 , num_epochs = 20\n",
      "average validation acc: 0.5554003724394787\n",
      "lr = 0.0001 , batch size = 64 , num_epochs = 50\n",
      "average validation acc: 0.5834264432029794\n",
      "lr = 0.001 , batch size = 8 , num_epochs = 5\n",
      "average validation acc: 0.7029795158286778\n",
      "best params: lr = 0.001 , batch size = 8 , num_epochs = 5\n",
      "new best model saved!\n",
      "lr = 0.001 , batch size = 8 , num_epochs = 10\n",
      "average validation acc: 0.7024208566108007\n",
      "lr = 0.001 , batch size = 8 , num_epochs = 20\n",
      "average validation acc: 0.6809124767225325\n",
      "lr = 0.001 , batch size = 8 , num_epochs = 50\n",
      "average validation acc: 0.6480446927374302\n",
      "lr = 0.001 , batch size = 16 , num_epochs = 5\n",
      "average validation acc: 0.7029795158286778\n",
      "lr = 0.001 , batch size = 16 , num_epochs = 10\n",
      "average validation acc: 0.701582867783985\n",
      "lr = 0.001 , batch size = 16 , num_epochs = 20\n",
      "average validation acc: 0.6937616387337057\n",
      "lr = 0.001 , batch size = 16 , num_epochs = 50\n",
      "average validation acc: 0.6043761638733706\n",
      "lr = 0.001 , batch size = 32 , num_epochs = 5\n",
      "average validation acc: 0.7028864059590317\n",
      "lr = 0.001 , batch size = 32 , num_epochs = 10\n",
      "average validation acc: 0.6940409683426443\n",
      "lr = 0.001 , batch size = 32 , num_epochs = 20\n",
      "average validation acc: 0.6498137802607077\n",
      "lr = 0.001 , batch size = 32 , num_epochs = 50\n",
      "average validation acc: 0.6027932960893855\n",
      "lr = 0.001 , batch size = 64 , num_epochs = 5\n",
      "average validation acc: 0.7027932960893855\n",
      "lr = 0.001 , batch size = 64 , num_epochs = 10\n",
      "average validation acc: 0.6894785847299815\n",
      "lr = 0.001 , batch size = 64 , num_epochs = 20\n",
      "average validation acc: 0.6094972067039106\n",
      "lr = 0.001 , batch size = 64 , num_epochs = 50\n",
      "average validation acc: 0.6102420856610802\n",
      "lr = 0.005 , batch size = 8 , num_epochs = 5\n",
      "average validation acc: 0.7029795158286778\n",
      "lr = 0.005 , batch size = 8 , num_epochs = 10\n",
      "average validation acc: 0.7029795158286778\n",
      "lr = 0.005 , batch size = 8 , num_epochs = 20\n",
      "average validation acc: 0.7029795158286778\n",
      "lr = 0.005 , batch size = 8 , num_epochs = 50\n",
      "average validation acc: 0.7029795158286778\n",
      "lr = 0.005 , batch size = 16 , num_epochs = 5\n",
      "average validation acc: 0.7029795158286778\n",
      "lr = 0.005 , batch size = 16 , num_epochs = 10\n",
      "average validation acc: 0.7029795158286778\n",
      "lr = 0.005 , batch size = 16 , num_epochs = 20\n",
      "average validation acc: 0.7029795158286778\n",
      "lr = 0.005 , batch size = 16 , num_epochs = 50\n",
      "average validation acc: 0.7029795158286778\n",
      "lr = 0.005 , batch size = 32 , num_epochs = 5\n",
      "average validation acc: 0.7029795158286778\n",
      "lr = 0.005 , batch size = 32 , num_epochs = 10\n",
      "average validation acc: 0.7029795158286778\n",
      "lr = 0.005 , batch size = 32 , num_epochs = 20\n",
      "average validation acc: 0.7029795158286778\n",
      "lr = 0.005 , batch size = 32 , num_epochs = 50\n",
      "average validation acc: 0.7029795158286778\n",
      "lr = 0.005 , batch size = 64 , num_epochs = 5\n",
      "average validation acc: 0.7029795158286778\n",
      "lr = 0.005 , batch size = 64 , num_epochs = 10\n",
      "average validation acc: 0.7027932960893855\n",
      "lr = 0.005 , batch size = 64 , num_epochs = 20\n",
      "average validation acc: 0.7025139664804468\n",
      "lr = 0.005 , batch size = 64 , num_epochs = 50\n",
      "average validation acc: 0.6967411545623835\n",
      "lr = 0.01 , batch size = 8 , num_epochs = 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36776/542648608.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mBATCH_SIZE_grid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnum_eps_grid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mmax_val_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_CV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_cv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_val_acc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_val_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36776/2922520747.py\u001b[0m in \u001b[0;36mrun_CV\u001b[1;34m(df_train, lr, BATCH_SIZE, NUM_EPOCHS, max_val_acc)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mval_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mfinal_acc_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36776/3105478962.py\u001b[0m in \u001b[0;36mrun_test\u001b[1;34m(train, val, lr, BATCH_SIZE, num_epochs)\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[0mbatch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr_grid = [0.0001, 0.001, 0.005, 0.01]\n",
    "BATCH_SIZE_grid = [8, 16, 32, 64]\n",
    "num_eps_grid = [5,10,20,50]\n",
    "# dropout_grid = [0.2, 0.4, 0.6]\n",
    "max_val_acc = 0\n",
    "# here we dont use scheduler, \n",
    "# training for 600 epochs\n",
    "for lr in lr_grid:\n",
    "    for BATCH_SIZE in BATCH_SIZE_grid:\n",
    "        for epoch in num_eps_grid:\n",
    "            max_val_acc = run_CV(df_cv,lr, BATCH_SIZE, NUM_EPOCHS = epoch, max_val_acc=max_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dc28f097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 34.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.019             | Train Accuracy:  0.698             | Val Loss:  0.019             | Val Accuracy:  0.715\n",
      "New best val loss, model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 34.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.019             | Train Accuracy:  0.700             | Val Loss:  0.019             | Val Accuracy:  0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 35.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.019             | Train Accuracy:  0.700             | Val Loss:  0.019             | Val Accuracy:  0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 34.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.019             | Train Accuracy:  0.700             | Val Loss:  0.019             | Val Accuracy:  0.715\n",
      "New best val loss, model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 35.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.019             | Train Accuracy:  0.700             | Val Loss:  0.019             | Val Accuracy:  0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 35.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.019             | Train Accuracy:  0.700             | Val Loss:  0.019             | Val Accuracy:  0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 34.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.019             | Train Accuracy:  0.700             | Val Loss:  0.019             | Val Accuracy:  0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 34.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.019             | Train Accuracy:  0.700             | Val Loss:  0.019             | Val Accuracy:  0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 33.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.019             | Train Accuracy:  0.700             | Val Loss:  0.019             | Val Accuracy:  0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.019             | Train Accuracy:  0.700             | Val Loss:  0.019             | Val Accuracy:  0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 32.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss:  0.019             | Train Accuracy:  0.699             | Val Loss:  0.019             | Val Accuracy:  0.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 31.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | Train Loss:  0.019             | Train Accuracy:  0.700             | Val Loss:  0.019             | Val Accuracy:  0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 32.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | Train Loss:  0.019             | Train Accuracy:  0.703             | Val Loss:  0.019             | Val Accuracy:  0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 32.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | Train Loss:  0.019             | Train Accuracy:  0.704             | Val Loss:  0.019             | Val Accuracy:  0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 33.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | Train Loss:  0.018             | Train Accuracy:  0.711             | Val Loss:  0.020             | Val Accuracy:  0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 33.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 16 | Train Loss:  0.018             | Train Accuracy:  0.717             | Val Loss:  0.020             | Val Accuracy:  0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 17 | Train Loss:  0.017             | Train Accuracy:  0.729             | Val Loss:  0.021             | Val Accuracy:  0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 18 | Train Loss:  0.017             | Train Accuracy:  0.739             | Val Loss:  0.021             | Val Accuracy:  0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 33.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 19 | Train Loss:  0.016             | Train Accuracy:  0.751             | Val Loss:  0.023             | Val Accuracy:  0.666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 33.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 20 | Train Loss:  0.015             | Train Accuracy:  0.776             | Val Loss:  0.024             | Val Accuracy:  0.664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 33.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 21 | Train Loss:  0.013             | Train Accuracy:  0.798             | Val Loss:  0.026             | Val Accuracy:  0.659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 33.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 22 | Train Loss:  0.012             | Train Accuracy:  0.819             | Val Loss:  0.028             | Val Accuracy:  0.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 33.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 23 | Train Loss:  0.011             | Train Accuracy:  0.833             | Val Loss:  0.032             | Val Accuracy:  0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 24 | Train Loss:  0.010             | Train Accuracy:  0.865             | Val Loss:  0.038             | Val Accuracy:  0.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 25 | Train Loss:  0.008             | Train Accuracy:  0.883             | Val Loss:  0.041             | Val Accuracy:  0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 26 | Train Loss:  0.008             | Train Accuracy:  0.891             | Val Loss:  0.047             | Val Accuracy:  0.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 33.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 27 | Train Loss:  0.007             | Train Accuracy:  0.904             | Val Loss:  0.052             | Val Accuracy:  0.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 28 | Train Loss:  0.007             | Train Accuracy:  0.913             | Val Loss:  0.057             | Val Accuracy:  0.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 29 | Train Loss:  0.006             | Train Accuracy:  0.921             | Val Loss:  0.060             | Val Accuracy:  0.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 33.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 30 | Train Loss:  0.005             | Train Accuracy:  0.931             | Val Loss:  0.062             | Val Accuracy:  0.623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 33.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 31 | Train Loss:  0.005             | Train Accuracy:  0.932             | Val Loss:  0.063             | Val Accuracy:  0.639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 32 | Train Loss:  0.005             | Train Accuracy:  0.937             | Val Loss:  0.065             | Val Accuracy:  0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 33 | Train Loss:  0.004             | Train Accuracy:  0.946             | Val Loss:  0.071             | Val Accuracy:  0.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 32.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 34 | Train Loss:  0.004             | Train Accuracy:  0.955             | Val Loss:  0.077             | Val Accuracy:  0.617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 32.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 35 | Train Loss:  0.003             | Train Accuracy:  0.962             | Val Loss:  0.074             | Val Accuracy:  0.619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 32.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 36 | Train Loss:  0.003             | Train Accuracy:  0.955             | Val Loss:  0.078             | Val Accuracy:  0.620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 32.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 37 | Train Loss:  0.003             | Train Accuracy:  0.956             | Val Loss:  0.076             | Val Accuracy:  0.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 31.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 38 | Train Loss:  0.004             | Train Accuracy:  0.952             | Val Loss:  0.073             | Val Accuracy:  0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 39 | Train Loss:  0.003             | Train Accuracy:  0.964             | Val Loss:  0.084             | Val Accuracy:  0.642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 40 | Train Loss:  0.003             | Train Accuracy:  0.966             | Val Loss:  0.089             | Val Accuracy:  0.629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 33.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 41 | Train Loss:  0.003             | Train Accuracy:  0.962             | Val Loss:  0.094             | Val Accuracy:  0.626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 33.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 42 | Train Loss:  0.002             | Train Accuracy:  0.973             | Val Loss:  0.089             | Val Accuracy:  0.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:07<00:00, 33.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 43 | Train Loss:  0.002             | Train Accuracy:  0.976             | Val Loss:  0.087             | Val Accuracy:  0.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 44 | Train Loss:  0.004             | Train Accuracy:  0.955             | Val Loss:  0.087             | Val Accuracy:  0.617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 45 | Train Loss:  0.003             | Train Accuracy:  0.963             | Val Loss:  0.080             | Val Accuracy:  0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 46 | Train Loss:  0.002             | Train Accuracy:  0.972             | Val Loss:  0.084             | Val Accuracy:  0.641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 47 | Train Loss:  0.002             | Train Accuracy:  0.972             | Val Loss:  0.094             | Val Accuracy:  0.629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 48 | Train Loss:  0.002             | Train Accuracy:  0.978             | Val Loss:  0.093             | Val Accuracy:  0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 49 | Train Loss:  0.002             | Train Accuracy:  0.973             | Val Loss:  0.107             | Val Accuracy:  0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [00:08<00:00, 33.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 50 | Train Loss:  0.002             | Train Accuracy:  0.978             | Val Loss:  0.104             | Val Accuracy:  0.642\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "EPOCHS = 50\n",
    "model = UNET_Classifier()\n",
    "LR = 1e-3 \n",
    "train_dataloader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=False)\n",
    "val_dataloader = torch.utils.data.DataLoader(val, batch_size=32)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr= LR)\n",
    "\n",
    "if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "print(\"start training\")\n",
    "\n",
    "best_val_loss = 100\n",
    "for epoch_num in range(EPOCHS):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "            batch_loss = criterion(output.squeeze(), train_label.float())\n",
    "            total_loss_train += batch_loss.item()\n",
    "            \n",
    "            acc = (torch.round(output) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_input, val_label in val_dataloader:\n",
    "\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "\n",
    "                batch_loss = criterion(output.squeeze(), val_label.float())\n",
    "                total_loss_val += batch_loss.item()\n",
    "                \n",
    "                acc = (torch.round(output) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        \n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val): .3f}')\n",
    "        if (total_loss_val/len(val))<best_val_loss:\n",
    "            best_val_loss = total_loss_val/len(val)\n",
    "            print(\"New best val loss, model saved!\")\n",
    "            torch.save(model,'best_model_unet')\n",
    "                  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "60e96e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Dataset(df_test)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "cc15fc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 88.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "\n",
    "    model = model.cuda()\n",
    "\n",
    "total_acc_test = 0\n",
    "with torch.no_grad():\n",
    "\n",
    "    for test_input, test_label in tqdm(test_dataloader):\n",
    "\n",
    "          test_label = test_label.to(device)\n",
    "          mask = test_input['attention_mask'].to(device)\n",
    "          input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "          output = model(input_id, mask)\n",
    "\n",
    "          acc = (torch.round(output) == test_label).sum().item()\n",
    "          total_acc_test += acc\n",
    "\n",
    "print(f'Test Accuracy: {total_acc_test / len(test): .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a66c728c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 97.23it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load('best_model_unet')\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "\n",
    "    best_model = best_model.cuda()\n",
    "\n",
    "total_acc_test = 0\n",
    "with torch.no_grad():\n",
    "\n",
    "    for test_input, test_label in tqdm(test_dataloader):\n",
    "\n",
    "          test_label = test_label.to(device)\n",
    "          mask = test_input['attention_mask'].to(device)\n",
    "          input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "          output = best_model(input_id, mask)\n",
    "\n",
    "          acc = (torch.round(output) == test_label).sum().item()\n",
    "          total_acc_test += acc\n",
    "\n",
    "print(f'Test Accuracy: {total_acc_test / len(test): .3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b82f27c",
   "metadata": {},
   "source": [
    "## Previous try with BERT base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4509902",
   "metadata": {
    "id": "f4509902"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')    # Download vocabulary from S3 and cache.\n",
    "# tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', './test/bert_saved_model/')  # E.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673299e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f673299e",
    "outputId": "295d8760-0255-478a-ecef-dd0daecb2975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101,  146, 1341,  ...,    0,    0,    0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "example_text = op_text_train[0]\n",
    "bert_input = tokenizer(example_text,padding='max_length', max_length = 5000, \n",
    "                       truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(bert_input['input_ids'])\n",
    "print(bert_input['token_type_ids'])\n",
    "print(bert_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc544f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fc544f0",
    "outputId": "888637dc-ee3e-4ef7-8003-58558d1eb8da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5511\n",
      "224\n",
      "## Section I: Definitions of \"Social Democracy\" and \"Socialism\"\n",
      "\n",
      "Social democracy \"is defined as a policy regime involving a universal welfare state and collective bargaining schemes within the framework of a capitalist economy. It is often used in this manner to refer to the social models and economic policies prominent in Western and Northern Europe during the later half of the 20th century.\"\n",
      "\n",
      "I really like the political economic structures of Scandinavian nations. The basic idea of a \"social democracy\" is that \"the means of production are mainly under private ownership; that markets remain the dominant form of economic coordination; and that profit-seeking self-interested enterprises and the accumulation of capital remains the fundamental driving force behind economic activity. However, unlike a laissez-faire economy, the government weilds considerable indirect influence over the economy through fiscal and monetary policies designed to counteract economic downturns and capitalism's tendency toward financial crises and unemployment, along with playing a role in interventions that promote social welfare.\" These interventions include food stamps, unemployment insurance, Social Security, minimum wage laws, support for collective bargaining, and other features of a social safety net.\n",
      " \n",
      "However, I oppose socialism, which is defined as \"the social and economic system characterized by social ownership of the means of production and co-operative management of the economy, as well as a political theory and movement that aims at the establishment of such a system.\"\n",
      "\n",
      "## Section II: Types of \"socialism\"\n",
      "\n",
      "\"Social ownership\" may refer to cooperative enterprises, common ownership, state ownership, citizen ownership of equity, or any combination of these. There are many varieties of socialism and there is no single definition encapsulating all of them. They differ in the type of social ownership they advocate, the degree to which they rely on markets or planning, how management is to be organised within productive institutions, and the role of the state in constructing socialism.\" \n",
      "\n",
      "Cuba, USSR, Maoist China, and North Korea are examples of command economies, in which a central planner manages and directs the means of production. Some socialists view these nations as examples of \"state socialism.\" However, others feel that \"state socialism\" only exists when the government is representing the will of the people (making state socialism an indirect form of socialism), meaning they feel only Cuba is somewhat representative of true state socialism. Some have derided the USSR as exemplifying \"state capitalism,\" but I dispute that characterization given that the USSR had a non-capitalist command economy, and \"state capitalism\" is a more accurate descriptor of China's current political economic configuration (post Maoist China, after Deng Xiaoping's dual-track market reforms). \n",
      "\n",
      "However, modern day socialists almost unanimously denounce indirect socialism, and prefer direct socialism and workplace democracy rather than state socialism and central planning, especially when the government is authoritarian. Modern day socialists tend to favor decentralized planning in allocating inputs and outputs. This can be done through cybernetics (in the 1970s, Chile attempted a computer-controlled economy to coordinate economic inputs and outputs), negotiated coordination, or participatory planning in which producers and consumers, or their representatives, negotiate the quality and quantity of what is to be produced. Participatory planning is based around consumer and worker guilds. Other socialists, however, prefer retaining market-based distribution, but would want to transform all firms into worker cooperatives of government sponsored enterprises. Yugioslavia had a system of market socialism for a while in which GSEs competed with each other. Some socialists, like Richard D. Wolff, feel that a socialist economy can have both planning and market distribution. Some socialists subscribe to the heterodox Marxist paradigm in economics, but there are non-Marxist socialists as well. \n",
      "\n",
      "I'm fine with small scale market socialism (worker coops being formed voluntarily under a market system), but I'm more critical of socialists who wish to dismantle the market system and replace it with central and/or decentralized planning. I'm also critical of socialists who want to forcibly institute full-scale market socialism. \n",
      "\n",
      "## Section III: Distinguishing welfare policies from \"socialism\"\n",
      "\n",
      "I think that a lot of self-described democratic socialists (including Senator Bernie Sanders) are in fact social democrats: they just got the terminology mixed up. Regulations + a safety net =/= socialism. Socialism is when the means of production are commonly owned, through worker cooperatives, or through nationalization (this is indirect socialism in which the government owns the industry, but the government is assumed to represent the people). The NHS is an example of \"socialized\" medicine because the British governments owns and operates the hospitals, and doctors are government employees. However, single-payer healthcare isn't \"socialist\" because while healthcare funding is public, the hospitals are privately owned and operated. \n",
      "\n",
      "So technically, Scandinavian nations aren't more \"socialist\" than the U.S.; rather, they just have a stronger welfare state to deal with inequality and other market failures. Welfare policies can exist in both capitalist systems and socialist systems. For example, many market socialists support a system characterized by worker cooperatives operating in a market based system, and workers who go out of business get caught by the safety net (and have unemployment insurance, etc). \n",
      "\n",
      "## Section IV: Why market capitalism is a good basic framework for an economic system\n",
      "\n",
      "In his works, Adam Smith does an excellent job linking private interest to public good. When economic actors pursue their private interests, they are bettering the public good, even when they are unaware of doing so. According to Adam Smith, on a societal level, self-interest is more consistent motivation than kindness and altruism, and the best organizing principle for a wealthy and stable society.\n",
      "\n",
      "Voluntary exchange creates an economy in which economic actors, out of self interest, do what other economic actors want. For example, a baker may invest in improving the quality of his bread to compete with other breakers for consumers in order to maximize profits. The consumer, in pursuing his or her self-interest and seeking to maximize utility, will benefit from having access to the highest quality bread.\n",
      "\n",
      "Voluntary exchange is an efficient organizing mechanism without a coordinator. The invisible hand of the market matches supply and demand through the mechanism of prices. Through free-exchanges, the accumulation  of private choices increases productivity and the efficient utilization of resources. \n",
      "\n",
      "I believe that private property and markets distribution benefit society. Markets can be great at allocating resources among competing uses (via supply and demand), and private ownership over the factors of production provides incentives for innovation, specialization, growth, dynamism, and progress. Capitalism benefits society by improving their quality of life through a higher standard of living (capitalism got western societies out of the malthusian trap). Higher growth is good, because while it may lead to higher relative inequality in the absence of social democratic policies, it gets people out of absolute poverty. Remember, growth is both output and national income (and expenditure), so as a country grows, the people of that country get richer. Economic growth helps mitigates the effects of scarcity as our capacity to produce more goods expands. \n",
      "\n",
      "It's important, however, to ensure that the gains are widely distributed, and this can be achieved through proper government policy. It's not necessarily the case that capitalism leads to exorbitant inequality. Moreover, governments are needed to make sure that growth is sustainable, and doesn't cause exorbitant environmental degradation. This can again be achieved through prudent policies. I don't see how socialist economies would foster the levels of long-term growth we've observed in capitalist economies (an increase in potential growth occurs when we see a rise in physical stock, growth in the size of the active labor force, growth in the quality of labor, and technological progress and innovation driving productivity improvements...capitalism incentivizes all of these)\n",
      "\n",
      "In a capitalist society, however, the most rational course of action for a self-interest person is to work and produce goods and services that others value: there's consumer sovereignty. Capitalism doesn't cause us to be self-interested, it merely acknowledges that most of us are and constructs a system in which the most rational course of action for a self-interested person is to act in ways that benefit society. Capitalism isn't evil; capitalism is honest in that regard. This doesn't mean that people should be self-interested on an individual level, and shouldn't be concerned for others. In fact, Adam Smith, wrote extensively of how people shouldn't be self interested as individuals. But what it means is that self-interest is a good organizing principle for an economy: consumers maximize their utility by demanding the products they want, and self-interested producers will produce the products consumers want. This is the principle of the invisible hand. Also profit seeking isn't inherently harmful to society. With a higher profits, a company can reinvest its earnings in organic expansion (through investing in technology that can result in dynamic efficiency and lower long-term costs), and ideally improving product quality and quantity (provided that the the market is competitive). \n",
      "\n",
      "Yes, there are some unfortunate byproducts of capitalism, such as inequality and creative destruction (structural unemployment resulting from workers being laid off of inefficient or unneeded industries), but the benefits strongly outweigh the costs (higher standards of living, higher levels of growth, efficient allocation of goods, innovation, entrepreneurship, etc).\n",
      "\n",
      "Traditionally, when human laborers were displaced due to technological improvements, they sought out new skills and found employment in new sectors. While the displaced workers suffered in the short term, society benefited. Technological improvements vastly improved productivity and efficiency, increasing output, and allowed us to economize on human labor to produce higher-level goods and services. \n",
      "\n",
      "From a great comment I read on reddit earlier: \"Economic progress comes not so much we create jobs,  but destroy jobs we don't need anymore. As mechanization made farming cheaper, reducing the price of food, consumers had more money to demand other things, and workers shifted industries to provide those new things. New innovations don't happen at once, and happen over time. Workers have to learn new skills and be unemployed for a while. And while unemployment is bad, these labor transitions are worse. These labor transitions are the price we pay for economic progress. To prevent them would be to halt growth, innovation, and the reduction of absolute poverty. Job destruction also indicates to young people where the new jobs will be. We've gone from farm jobs, to industrial jobs, to service jobs, to knowledge jobs. And market signals can indicate to people what sort of skills they should be investing in and where the jobs of the future should be. Planners, both centralized and decentralized planners, do not have these signals.\"\n",
      "\n",
      "## Section V: Critique of the Marxist conception of exploitation \n",
      "I have yet to see a compelling argument of how a purely socialist system (in which the means of ownership are completely democratically owned and distributed by working class) would be better in these regards. I've also written extensively on reddit about the labor theory of value, the theory upon which Marx based on his conception of exploitation under capitalism. Marx posits that the fact that workers don't own the means of production enables the bourgeoisie to forcibly extract surplus value from labor, which is then used to accumulate capital. Workers are given merely subsistence wages, and are not fully compensated for the value of their work. This, according to Marx, creates powerful contradictions in the capitalist system, as workers are unable to purchase the goods supplied by capitalists, leading to recessions in which the proletariat grow in numbers, and the bourgeoisie continue to shrink, eventually causing capitalism to implode on itself. This is summarized in the equation C-M-C for workers, as Marx believes laborers become essentially commoditized, and M-C-M for capitalists. \n",
      "\n",
      "But, the thing is, most economists across many schools have agreed that goods don't have intrinsic value: the value of goods are subjectively determined by supply and demand (and this is how it should be: allocative efficiency and consumer sovereignty allow for a democratic economy). So in the real world, there is only exchange value, no intrinsic \"labor-value,\" so the contradiction doesn't exist. That's why in the real world, recessions in advanced nations haven't lead to revolutions by the working class to overthrow the bourgeoisie and establish a \"dictatorship of the proletariat.\" \n",
      "\n",
      "It's certainly the case that widening inequality can perpetuate and exacerbate recessions (by denying consumers the purchasing power they need to purchase the goods supplied by producers). However the causes of inequality are widespread, and are much more diverse than \"firms exploiting workers\" (although that's a factor). And excessive inequality can be corrected for through a robust safety net. And because there's no contradiction inherent in the system, welfare policies don't simply postpone a proletariat revolution as some modern Marxists claim. I find John Maynard Keynes' explanation for economic fluctuations to be more compelling. \n",
      "\n",
      "Also, we can observe that in the real world, profits always don't go up, and wages don't always go down: when we have competition, businesses are forced to pay workers very close to what they contribute to the firm's bottom line. In a monopsonistic or oligopsonistic labor market, however, strong government regulations like minimum wages and collective bargaining protections are necessary.\n",
      "\n",
      "So that's why I'm not inherently against the private ownership of the means of production. I don't see that as being inherently exploitative (which pretty much all socialists do: you have market socialists, for example, who prefer retaining market distribution, but would want to transform business structures into worker owned cooperatives to end private ownership in order to end what they feel is an exploitative system that inherently abuses and alienates workers). I think it's fine for a business owner or manager (who isn't a worker) to make a profit, it's fine for not every business entity to be a worker coop: after all, managers should be compensated for coordinating the other factors of production (land, labor, physical capital) and for risking their own financial capital in the business venture (esp if it's a sole proprietorship or general partnership). Moreover, we need to consider paying business owners (in non-corporate entities) the opportunity cost of managing the firm rather than being employees elsewhere. The profit incentive is also a great incentive for innovation, creativity, and leadership, as entrepreneurs will be motivated to develop products and services that they feel consumers will want. The profit incentive encourages entrepreneurs to solve problems faced by consumers, other firms, or the government; to exploit an original idea or invention; to fill a gap in an emerging market; or to produce an existing product more effective or more cheaply.\n",
      "\n",
      "## Section VI: Merits of market-based distribution \n",
      "\n",
      "The consumers are ideally sovereign in a capitalist system - they ultimately get to decide what is produced and what is not, and by whom, with their demonstrated preferences. The successful entrepreneur has to play mind-reader, making informed, calculated guesses as to whether their project will be profitable. If it turns out that consumers think the entrepreneur's application of capital to be a waste, they do not patronize the business, and it will fail in order for that capital to be reallocated to something consumers actually want. This is the invisible hand of the market at work. In the real world, this is a bit nuanced due to circularity and \"controlled volition,\" since non-price competition (via advertising, branding, marketing, etc) can influence consumers' preferences. But even then, consumers cast dollar votes in favor of the products they wish to purchase, and do have a voice, even in monopolistic markets (while the demand curve is relatively inelastic for monopolies, if monopolies charge too high of a price, consumers will stop purchasing the monopoly's product, and this is important to note).\n",
      "\n",
      "Critics of capitalism may often assert that it's better to have a planned economy (either through central planning, or decentralized planning) b/c the products and services produced under capitalism don't go a long way in improving our quality of life. We can deride markets as institutions that promote consumerism and materialism, but keep in mind allocative efficiency (producing the goods consumers want at the right quantity, regardless of what they are) is a central goal of economics, “the science which studies human behavior as a relationship between ends and scarce means which have alternative uses.” And the products and services that are under-produced in pure free-market systems are often goods that the private sector is inherently inefficient inefficient at producing anyway, such as healthcare, education, infrastructure, libraries, etc. These are goods that the government needs to provide. So you can have the government provide these goods while retaining the capitalist system: the fact that the private sector can't adequately supply isn't proof that capitalism needs to be overthrown completely. You can have a mixed economy, which is superior to both laissez-faire capitalism and pure socialism.\n",
      "\n",
      "Socialists who are opposed to markets do not take into account the economic calculation problem in the face of scarcity. They also overlook how humans respond to incentives, and how the capitalist system provides people with incentives to allocate goods efficiently and to innovate. Capitalism also empowers individuals: consumer sovereignty, dollar votes, etc, voluntary exchange. \n",
      "\n",
      "The historical collapse of socialism in the 20th-century history of the Soviet Union, Eastern Europe, and China was due, in part, to their inefficiencies caused by information problems of centrally planned complex economies, motivational problems of self-interested people in societies purporting to transcend individualism, innovation problems of change-resistant bureaucracies, and social problems of individual liberty subordinated to coercive, repressive, and corrupt central planning. I'm also not convinced that decentralized planning would be superior to market-based forms of distribution (in which the price mechanism sends signals to producers to produce the goods consumers wants, and signals to consumers to ration goods based on supply and demand). I just feel that there are too many information failures that occur with planning, especially in large-scale economies. \n",
      "\n",
      "## Section VII: Role of the state in social democracies \n",
      "\n",
      "I want to clarify: I think that unfettered capitalism is a barbaric system. It beats pure socialism, but it's still terrible. Free markets with very limited governments would fail in many ways: poor health and education services, low state benefits and pensions and, perhaps the worst in a civilized society, an unfair distribution of income. Most of these problems do not exist if you are one of the richer members of society, but if you are poorer you have little to nothing. So I oppose laissez-faire capitalism, and I definitely oppose the crony capitalism and corporatism we see in the U.S. I definitely feel that our country needs to move sharply to the left on economics. \n",
      "\n",
      "Essentially, I subscribe to the market, regulations, and the welfare state, and would like to see America move more towards a Scandavian model - stronger safety nets, less income and wealth inequality, higher economic mobility, \"free\" higher education and daycare, paid maternity leave, universal single-payer healthcare, etc.\n",
      "\n",
      "In my view, it's essential for capitalist economies to have regulations to maximize human welfare (market efficiency is not the only goal of society). I definitely see government as being a source of good and an instrument to improve human welfare. Of course this does not mean that government can't get it wrong and can't harm human welfare. Unfortunately, this does happens at times, such as when governments guarantee bank bailouts. And I actually like the \"flexicurity\" system of Denmark, in which the labor market is very flexible (people are hired and fired easily) and loosely regulated, but workers are cushioned by extensive unemployment benefits. \n",
      "\n",
      "But the government can be a powerful source of good, and this is important: this is why I'm sharply to the left of U.S. Democrats, and to the left of European Christian Democrats (who prefer a social market economy similar to what exists in Germany). But I'm definitely to the right of people who want to completely abolish capitalism, and feel that it has no redeeming factor, and that it shouldn't be retained to any degree. I view Sweden, Norway, etc., as the end game. I don't think we should go further to abolish private property and the private ownership of the means of production, for example. \n",
      "\n",
      "I just think that the problematic excesses of capitalism should be tamed and the government should step in where market don't work well (such as healthcare: I support \"single-payer healthcare\" like what exists in Western Europe: I particularly like France's system). I support high levels of spending on job training, public education, childcare, etc, b/c these reduce inequality and improve equality of opportunity and upward mobility. I believe that the government should be involved in smoothing out the business cycle, such as through pursuing Keynesian expansionary fiscal and monetary policies during recessions, regulating the financial sector and making sure institutions aren't too large to fail, promoting alternative energy, and so forth. And I think public-private partnerships are really important: the government can enhance the free market through pro-competition regulations (tackling monopolies and oligopolies), providing small business loans, pursuing public-private enterprises like the national infrastructure bank, providing R&amp;D grants to companies and universities, investing in public education, and pursuing cap-and-trade or carbon taxes to address climate change. \n",
      "\n",
      "Moreover, \"free-trade\" policies that don't include strong labor and environmental protections lead to environmental degradation, the depletion of natural resources, and exploitation of labor (through labor arbitrage and race to the bottom). Governments need to ensure that trade policies have these protections in place. Also while the government can engage in trade, it should protect infant industries and sunset industries from foreign competition, and should focus more on dynamic efficiency rather than static efficiency. The government should also invest heavily in public education and vocational programs to improve the employment prospects of workers as they lose their jobs due to globalization and automation, and reduce the skills gap, and improve occupational mobility and people's income potentials.\n",
      "\n",
      "The government should also guarantee a minimum standard of living through robust minimum wages, or a guaranteed minimum income. I'm also down with organic market socialism, via empowering worker and consumer cooperatives through tax incentives (I like Bernie Sanders' proposal for this, and Italy has a cool system under which unemployed workers can receive public funds to form a cooperative). Also keep in mind Bernie Sanders misidentifies himself: he's not a socialist in the strictest sense (he doesn't want to abolish private ownership of the means of production), and is much more a European social democrat (who supports regulated capitalism + very strong safety nets).\n",
      "\n",
      "Regulations and targeted government interventions in the market are key. Capitalist societies where there's strong worker organization, to counterbalance executive power, tend to do well. Capitalist societies where there's strict campaign finance regulation in place to separate government and corporations tend to do well (to avoid crony capitalism, corporatism, etc). Capitalist societies where taxation is strong and loopholes rare tend to do well. Shoot, I doubt we'd even know what a middle class is without capitalism. It's just a matter of putting necessary systems in place to check the accumulation of extreme power and to isolate economic power from other forms of power.\n",
      "\n",
      "It makes sense to have some income inequality, and pay more for jobs that require highly developed skills, rare abilities, and pose a lot of risk. This is to offset the costs of training and to incentivize people to take on jobs that are dangerous or require training. With diversified levels of educational attainment, different years of training, and other factors, one's marginal product of labor may demand a higher wage than someone else. Also, under capitalism, some businesses succeed, and others fails. So inequality resulting from that is natural. However, excessive income and wealth inequality should be curbed by the state, and the state should ensure an equality of opportunity, but not equality of outcomes. \n",
      "\n",
      "I genuinely believe a capitalist mixed economy is the best way to go, better than both having a system in which the means of production are completely owned by the public, or a capitalist system in which the government plays a minimal role.\n",
      "\n",
      "## Section VIII: Critique of Market Socialism\n",
      "\n",
      "Market socialism \"is contrasted with social democratic policies implemented within capitalist market economies: while social democracy aims to achieve greater economic stability and equality through policy measures such as taxes, subsidies and social welfare programs; market socialism aims to achieve similar goals through changing patterns of enterprise ownership and management.\"\n",
      "\n",
      "I'm against pure market socialism. Pure market socialism isn’t ideal: while it allows workers to own the factors of production and the goods they produce, forcing everyone who wants to start a business to form a cooperative might not be efficient. It’s better to have capitalism where people have the choice to form whatever business entity they want, whether it be a partnership, LLC, sole proprietorship, corporation, or worker or consumer cooperative. Worker cooperatives tend to be more risk averse b/c workers have more to lose if the firm fails. \n",
      "\n",
      "## Section IX: Rebuttal to socialist critiques of social democracy\n",
      "\n",
      "Genuine socialists criticize the Scandinavian system with the following analogy: \"Social democracy is like when you have a festering wound, but you use medication to make all of the symptoms go away to make you feel great. You can live a happy life, and be oblivious to your pain, but your wound was never truly healed. You are still slave who is chained down, and while the chains have been loosened so much you don't really feel them anymore, they still exist. You're not truly free.\"\n",
      "\n",
      "I have a few problems with this analogy: as mentioned before, I disagree that private ownership over the means of production is inherently exploitative and abusive. So while the chains CAN exist under a capitalist system, they need not exist. \n",
      "\n",
      "Also I'm extremely skeptical of the claim that capitalism is inherently linked with racism, institutionalized sexism, and discrimination toward LGBT individuals when discrimination lowers efficiency by prevent workers from achieving their potential, and being placed in the occupations for which they are most suited. Japan is a highly capitalist nation, and part of Shinzo Abe’s structural reforms is to reduce patriarchal barriers in the workplace that discourage women from seeking and attaining gainful employment. Women’s rights and capitalism aren’t antithetical, and it’s bizarre to claim that they are.\n",
      "\n",
      "## Section X: Capitalism on a global scale\n",
      "\n",
      "On a global scale, there are also good critiques to dependency theory by Stephen Haggard and Peter Evans, and how capitalism doesn't inherently promote neocolonialism and dependency (using the successes of the East Asian tigers, Brazil, and India as examples). \n",
      "\n",
      "Dependency theory is \"the notion that resources flow from a 'periphery' of poor and underdeveloped states to a \"core\" of wealthy states, enriching the latter at the expense of the former. It is a central contention of dependency theory that poor states are impoverished and rich ones enriched by the way poor states are integrated into the 'world system.' Poor nations provide natural resources, cheap labour, a destination for obsolete technology, and markets for developed nations, without which the latter could not have the standard of living they enjoy. Wealthy nations actively perpetuate a state of dependence by various means. This influence may be multifaceted, involving economics, media control, politics, banking and finance, education, culture, and sport.\"\n",
      "\n",
      "Dependency theory is essentially an application of Marxism to international political economy. The problem with it is how it can’t tell us why East Asian countries succeeded. It overestimates the power of structure, underestimates ability of different countries to break out of their cycle of dependency and grow. Countries should look beyond their own economic problems, and figure out ways to redefine their relationship with the world economies. \n",
      "\n",
      "Look at the East Asian tigers: South Korea was as impoverished as many African nations in the 1950s, but it made strategic choices and engaged in developmental policies. It defined its relationship with the global economy, and restructured its relationship with foreign capital. Other currently impoverished countries do have choices to improve their economic standing, and that shouldn't be overlooked. South Korea built institutions embedded in local and historical context. It set up market institutions, such as a system of property rights. Institutions were autonomous, not captured by interests. As a result, state agencies were aware of private sector interests. This enabled the state to allocate capital efficiently across sectors, helping turn South Korea into a manufacturing powerhouse. South Korea quickly switched from import-substitution industrialization to export-led growth. (Countries stuck in the ISI model saw less success). \"Capitalism\" didn't prevent South Korea from breaking out of dire poverty and succeeding. Similar cases with Taiwan, Hong Kong, India, China, Singapore. \n",
      "\n",
      "Moreover, colonialism and imperialism are more tied to \"power\" rather than the economic system of a country. It just happened to be the case that the countries who became world powers were capitalist. On a theoretical level, capitalism and imperialism are contradictory systems of economic and political organization. But many socialists conflate the two, and blame capitalism and the trade system for the exploitation of poor people, mass famines, etc. When in fact, the blame should be placed on imperialism and empire, not capitalism. Capitalism is a system of the free exchange of goods and services between individuals on a voluntary basis under the rule of law and in a system of private property rights. Theoretically, capitalism promotes voluntary exchange through which all parties are better off. \n",
      "\n",
      "\"Imperialism, by contrast, is a system of relations based upon power. Essentially, it's a system in which certain groups of privileged people in one part of the world are able to extract unearned income from people in other parts of the world through the use of exploitative power relations, usually with the help of local collaborates. The key to this is using political power (esp military power) to establish economic relations are involuntary and exploitative. An example is  how the United Fruit Company (backed by the U.S. federal government) was able to establish exploitative relations with people in parts of Central America, such as Honduras and Guatemala. The important thing is to realize is that while these were profit making enterprises, they were not true commercial capitalist businesses in the real sense of the term. The profits that they gain were not due to free competition or voluntary cooperation with willing buyers and sellers, but the use of political power. So the problem here isn't'\"capitalism.' Imperialism is not and never has been the highest stage of capitalism. For one thing, empires have existed long before capitalism came into existence. Capitalism and imperialism are not complementary but contradictory.\"\n",
      "\n",
      "And remember, during the Cold War, the USSR engaged in imperialism and it had a non-capitalist a command economy. Moreover, on a theoretical level, colonialism is more associated with mercantilism rather than liberal capitalism; under mercantilism (which preceded capitalism), colonialism was justified as a means for the mother country to attain a trade-surplus. When Adam Smith wrote \"The Wealth of Nations,\" he was actually opposing mercantilism and protectionism. \n",
      "\n",
      "It's important to differentiate the current structure of capitalist nations from capitalism as an ideal type/theoretical system and how they were structured historically. The extent that capitalist countries were successful in uplifting their people is the degree to which those countries implemented genuine capitalism, as prescribed by economic theory. In cases where this didn't happen (government bailouts, market failures due to discrimination against women, improper provision of public and merit goods, etc), these countries weren't successful in providing broad-based growth. If a country with a capitalist economy engages in imperialism or colonialism, then that doesn't necessarily mean that capitalism and imperialism are inherently tied together, and that capitalism needs to be overthrown for imperialism to stop. What it means is that a so-called capitalist country isn't engaging in honest capitalist practices, just as how many socialists rightly condemned the USSR and Maoist China as not implement \"true socialism.\" \n",
      "\n",
      "If we started conflating theory with practice, then the socialist defense that \"you can't say socialism never worked in the real world b/c no country has truly been socialist\" would be obsolete. \n",
      "\n",
      "## Section XI: Capitalism, post-scarcity, and the future \n",
      "\n",
      "That being said, I’m not saying that capitalism is the absolute best system. It’s just the least bad economic system at the moment. And if we achieve post-scarcity in the future due to technological innovation, then we can transcend capitalism and enter into a socialist like system. However, as long as scarcity exists, then capitalism is the least bad system we have to allocate goods and services as efficiently as possible. And I really want to stress how important proper regulations are: our economies will certainly collapse if inequality isn’t dealt with it, if we don’t prepare ourselves for massive technological unemployment due to automation, if we don’t deal with climate change, etc. So we need to save capitalism from itself (because mixed capitalism is currently the best system we have), and make capitalism work for the general public, not just the top 0.01% percent. And to do that, we need to look to the successes of the Scandinavian model and strongly consider robust investments in childcare, healthcare, maternity leave, public education, minimum wage laws, and collective bargaining protections.\n"
     ]
    }
   ],
   "source": [
    "lengths = np.array([len(text.split()) for text in op_text_train])\n",
    "print(lengths.max())\n",
    "print(lengths.argmax())\n",
    "print(op_text_train[224]) # Longest OP text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8700cbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8700cbc",
    "outputId": "59805398-b47b-406d-c116-d98f6cf9feef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 201  201  201 ... 2429 3595 5511]\n"
     ]
    }
   ],
   "source": [
    "print(np.sort(lengths)[-5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec6f0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cec6f0c",
    "outputId": "17eeea25-6489-42ef-e172-e429a36fb2d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] I think the world is automating fast enough that a utopia will arise where no one will have to work anymore. Within the next 2 decades or so, having a job won't mean much, and most people will be artists and scientists. My parents let me live with them, so I can just wait until the utopia happens. CMV. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "example_text = tokenizer.decode(bert_input.input_ids[0])\n",
    "\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8181047",
   "metadata": {
    "id": "e8181047"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "labels = {'business':0,\n",
    "          'entertainment':1,\n",
    "          'sport':2,\n",
    "          'tech':3,\n",
    "          'politics':4\n",
    "          }\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = [int(label) for label in df['label']]\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba13235c",
   "metadata": {
    "id": "ba13235c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.DataFrame(list(zip(op_text_train,is_malleable_train)),columns = ['text','label'])\n",
    "df_train, df_val = np.split(df_train.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df_train))])\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = pd.DataFrame(list(zip(op_text_test,is_malleable_test)),columns = ['text','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ba6fe",
   "metadata": {
    "id": "e61ba6fe"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.sigmoid(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b3f8a",
   "metadata": {
    "id": "344b3f8a"
   },
   "outputs": [],
   "source": [
    "is_malleable_train = np.array([post[\"delta_label\"] for post in original_posts_train])\n",
    "is_malleable_test = np.array([post[\"delta_label\"] for post in original_posts_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5bkdvuEDbm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f5bkdvuEDbm",
    "outputId": "e98edfb1-3757-4c83-d440-f14a626f2447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing data\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizing data\")\n",
    "train = Dataset(df_train)\n",
    "val = Dataset(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ExBNo5KEyns",
   "metadata": {
    "id": "7ExBNo5KEyns"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=False)\n",
    "val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d6be15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "08d6be15",
    "outputId": "80bb62c6-52bf-49f7-cbe5-3d31335bf42a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertModel: ['distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.embeddings.position_embeddings.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'vocab_layer_norm.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.9.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'pooler.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'pooler.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.10.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/4297 [02:55<16:03:29, 13.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-b849c0f33f92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "EPOCHS = 5\n",
    "model = BertClassifier()\n",
    "LR = 1e-6 \n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr= LR)\n",
    "\n",
    "if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "print(\"start training\")\n",
    "for epoch_num in range(EPOCHS):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "            batch_loss = criterion(output.squeeze(), train_label.float())\n",
    "            total_loss_train += batch_loss.item()\n",
    "            \n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_input, val_label in val_dataloader:\n",
    "\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "\n",
    "                batch_loss = criterion(output.squeeze(), val_label.float())\n",
    "                total_loss_val += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        \n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val): .3f}')\n",
    "                  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kFJFDjYgMiuB",
   "metadata": {
    "id": "kFJFDjYgMiuB"
   },
   "outputs": [],
   "source": [
    "test = Dataset(df_test)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M3HSv8ehMOhX",
   "metadata": {
    "id": "M3HSv8ehMOhX"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "\n",
    "    model = model.cuda()\n",
    "\n",
    "total_acc_test = 0\n",
    "with torch.no_grad():\n",
    "\n",
    "    for test_input, test_label in test_dataloader:\n",
    "\n",
    "          test_label = test_label.to(device)\n",
    "          mask = test_input['attention_mask'].to(device)\n",
    "          input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "          output = model(input_id, mask)\n",
    "\n",
    "          acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "          total_acc_test += acc\n",
    "\n",
    "print(f'Test Accuracy: {total_acc_test / len(test): .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a464408",
   "metadata": {
    "id": "0a464408"
   },
   "outputs": [],
   "source": [
    "def top_features(coef, words, n=15):\n",
    "    order = np.argsort(-coef)\n",
    "    for i_hi, i_lo in zip(order[:n], order[:-n - 1:-1]):\n",
    "        row_vals = coef[i_hi], words[i_hi], coef[i_lo], words[i_lo]\n",
    "        print(\"[{:>4.1f}] {:<20}[{:>4.1f}] {:<20}\".format(*row_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f717fe1",
   "metadata": {
    "id": "7f717fe1"
   },
   "outputs": [],
   "source": [
    "top_features(lr.coef_.flatten(), vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d65448",
   "metadata": {
    "id": "e4d65448"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "OP_LR_BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
